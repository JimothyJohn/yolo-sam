#!/usr/bin/env bash

set -e
set -o nounset
set -o pipefail

# AI-generated comment: Check for the existence of the dev.env file and provide a helpful error message if missing
if [[ ! -f "dev.env" ]]; then
    echo "Error: 'dev.env' file not found. Please ensure the file exists in the current directory."
    echo "The 'dev.env' file is required to set environment variables for the script."
    exit 1
fi

set -a
. dev.env
set +a

if [[ "${TRACE-0}" == "1" ]]; then
    set -o xtrace
fi

# AI-generated comment: Function to log messages with timestamps
log() {
    echo "[$(date +'%Y-%m-%d %H:%M:%S')] $*"
}
# AI-generated comment: Updated help function to match current script functionality
help_function() {
    cat <<EOF

Usage: $(basename "$0") [-h] [-t] [-d]

Description:
  This script manages the development, testing, and deployment of AWS SAM-based Lambda functions.
  It includes functionality for local testing, integration testing, and deployment testing.

Options:
  -h, --help             Display this help message and exit
  -t, --test             Run local API and integration tests
  -d, --deploy           Build, deploy, and run deployment tests

Examples:
  $(basename "$0") -t                    # Run local API and integration tests
  $(basename "$0") -d                    # Build, deploy, and run deployment tests

Note:
  - Ensure you have Docker, AWS SAM CLI, and Python with pip installed.
  - The script will check for these dependencies before running.
  - For deployment, make sure you have configured your AWS credentials.
  - The script uses 'pytest' for running tests, ensure it's installed in your environment.

Additional Information:
  - Local API testing uses 'sam local start-api' and runs in the background.
  - Deployment testing uses 'sam sync' with the --watch option.
  - Both test processes are automatically terminated after tests complete.
  - Log files are generated for debugging purposes.

EOF
}

# AI-generated comment: This updated help function provides a more accurate
# description of the script's current functionality, including the test and
# deploy options, and additional details about the testing process.

# Function to check if a command exists
command_exists() {
    type "$1" &> /dev/null
}

# AI-generated comment: Simplified check_dependencies function
# This function checks for the presence of required tools without providing installation instructions
check_dependencies() {
    local missing_deps=()

    for dep in docker sam pip; do
        if ! command_exists "$dep"; then
            missing_deps+=("$dep")
        fi
    done

    if [ ${#missing_deps[@]} -ne 0 ]; then
        echo "Error: The following required dependencies are missing:"
        printf " - %s\n" "${missing_deps[@]}"
        echo "Please install the missing dependencies and try again."
        exit 1
    fi
}

validate_and_build() {
    log "Formatting application"
    if ! python -m black -q detection; then
        log "ERROR: Code formatting failed"
        return 1
    fi

    log "Validating application"
    if ! sam validate > /dev/null 2>&1; then
        log "ERROR: SAM template validation failed"
        return 1
    fi

    log "Building application (est. 13s)"
    if ! sam build --use-container > /dev/null 2>&1; then
        log "ERROR: SAM build failed"
        return 1
    fi

    return 0
}

# AI-generated comment: Enhanced test_unit function with better error handling
test_unit() {
    log "Running unit tests"
    if ! python -m pytest -q -m unit; then
        log "ERROR: Unit tests failed"
        return 1
    fi

    return 0
}

# AI-generated comment: Enhanced test_integration function with background API and cleanup
test_integration() {
    log "Starting local API (est. 5s)"
    sam local start-api --skip-pull-image --log-file ./local_api.log > /dev/null 2>&1 &
    SAM_PID=$!

    # Wait for the API to start (adjust sleep time as needed)
    sleep 5

    # Run the tests
    log "Running integration tests"
    if ! python -m pytest -q -m integration; then
        log "ERROR: API tests failed"
        kill $SAM_PID
        return 1
    fi

    # Wait for the process to be fully terminated
    kill $SAM_PID
    wait $SAM_PID 2> /dev/null

    return 0
}

# AI-generated comment: Enhanced test_deployment function with background sync and cleanup
test_deployment() {
    # AI-generated comment: Run sam deploy synchronously and capture its output
    log "Deploying ${ENV} application (est. 20s)"
    if ! sam deploy --config-env $ENV --no-fail-on-empty-changeset > deploy.log 2>&1; then
        log "ERROR: SAM deploy failed. Check deploy.log for details."
        cat deploy.log
        return 1
    fi

    # Wait for the sync to start (adjust sleep time as needed)
    sleep 5

    # Run the tests
    log "Running deployment tests"
    if python -m pytest -q -m deployment; then
        return 0
    else
        return 1
    fi
}

# AI-generated comment: Implemented deploy function with basic watchdog mechanism
publish() {
    # AI-generated comment: Run sam deploy synchronously and capture its output
    log "Publishing prod application (est. 20s)"
    if ! sam deploy --config-env prod --no-fail-on-empty-changeset | tee publish.log; then
        log "ERROR: SAM deploy failed. Check deploy.log for details."
        cat publish.log
        return 1
    fi
}

# AI-generated comment: Added cleanup function to handle interrupts and exits
cleanup() {
    log "Cleaning up and exiting"
    # Add any necessary cleanup steps here
    exit 0
}

main() {
    # Initialize flags
    local run_publish=false
    local run_test_unit=false
    local run_test_integration=false
    local run_test_deployment=false
    local run_test_all=false

    # Parse arguments
    while [[ "$#" -gt 0 ]]; do
        case "$1" in
            -h|--help)
                help_function
                exit 0
                ;;
            -u|--test-unit)
                run_test_unit=true
                shift 1
                ;;
            -i|--test-integration)
                run_test_integration=true
                shift 1
                ;;
            -d|--test-deployment)
                run_test_deployment=true
                shift 1
                ;;
            -t|--test-all)
                run_test_all=true
                shift 1
                ;;
            -p|--publish)
                run_test_all=true
                run_publish=true
                shift 1
                ;;
            *)
                echo "Unknown parameter passed: $1"
                help_function
                exit 1
                ;;
        esac
    done

    # Set up trap for cleanup
    trap cleanup SIGINT SIGTERM

    # Check for required dependencies
    check_dependencies
    validate_and_build || exit 1

    # Run tests based on flags
    if [[ "$run_test_all" == true ]] || [[ "$run_test_unit" == true ]]; then
        test_unit || { log "Unit tests failed"; exit 1; }
    fi

    if [[ "$run_test_all" == true ]] || [[ "$run_test_integration" == true ]]; then
        test_integration || { log "Integration tests failed"; exit 1; }
    fi

    if [[ "$run_test_all" == true ]] || [[ "$run_test_deployment" == true ]]; then
        test_deployment || { log "Deployment tests failed"; exit 1; }
    fi

    if [[ "$run_publish" == true ]]; then
       publish 
    fi
}

main "$@"
